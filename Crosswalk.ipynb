{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인공지능 기술을 활용한 소셜임팩트 창출 프로젝트 presented by JohnKim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"JN Images\\process.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'blue' > [ Problem Scoping ] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4Ws Problem Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Statement Template (******* Example)\n",
    "- <font color = 'darkpurple' > 1W: Who is having the problem? Who are the stakeholders? What do you know about them! </font>\n",
    "<p> .... </p><br>\n",
    "- <font color = 'darkpurple' > 2W: What is the nature of the problem? What is the problem? How do you know it is a problem? </font>\n",
    "<p> ....</p><br>\n",
    "- <font color = 'darkpurple' > 3W: Where does the problem arise? What is the context/situation in which the stakeholders experience the problem? </font>\n",
    "<p> .... </p> <br>\n",
    "- <font color = 'darkpurple' > 4W: Why do you believe it is a problem worth solving? What would be of key value to the stakeholders? How would it improve their situation? </font>\n",
    "<p> ....     </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <a href= 'https://ncase.me/loopy/' > system map 그리기 </a>\n",
    "- <a href= 'https://www.cockos.com/licecap/' > gif 파일로 저장하기 </a> <br><br>\n",
    "<hr style = \"border-width: 10px;\" color = \"red\"><hr style = \"border-width: 10px;\" color = \"red\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'blue' > [ Data Acquisition ] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources Of Data \n",
    "1. We will create our own data by capturing many images with our webcam!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e066c7e60a56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mimutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperspective\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfour_point_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imutils'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "from imutils.perspective import four_point_transform\n",
    "import imutils\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code creates the function <font color = 'blue' >  <b>findTrafficSign</b></font>. \n",
    "<p><font color = #C71585>First, the function find blobs within the defined hsv range on the image. In this case, we are looking for a black blob.<br><br>\n",
    "    <code>lower_hsv = np.array([0,0,0])</code><br><code>upper_hsv = np.array([90,100,100])</code><br><br>Second, the function finds the biggest blob.<br> Third, the code draws a line around the largest blob. \n",
    "    <br> Finally, the function creates a cropped image of the largest blob</font></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------q--------------------------------------------------------------\n",
    "# Function: To detect Traffic Sign\n",
    "# Inputs:   \n",
    "# return:   \n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "def findTrafficSign(grabbed, frame):\n",
    "     \n",
    "    # define range HSV for blue color of the traffic sign\n",
    "    lower_hsv = np.array([0,130,100])\n",
    "    upper_hsv = np.array([10,255,255])\n",
    "    \n",
    "    # frame = imutils.resize(frame, width=500)\n",
    "    frameArea = frame.shape[0]*frame.shape[1]\n",
    "    \n",
    "    # convert color image to HSV color scheme\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # define kernel for smoothing   \n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    # extract binary image with active the hsv regions\n",
    "    mask = cv2.inRange(hsv, lower_hsv, upper_hsv)\n",
    "    # morphological operations\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # find contours in the mask\n",
    "    cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]\n",
    "       \n",
    "    # define variables to hold values during loop\n",
    "    largestArea = 0\n",
    "    largestRect = None\n",
    "    \n",
    "    # only proceed if at least one contour was found\n",
    "    if len(cnts) > 0:\n",
    "        for cnt in cnts:\n",
    "            rect = cv2.minAreaRect(cnt)\n",
    "            box = cv2.boxPoints(rect)\n",
    "            box = np.int0(box)\n",
    "            \n",
    "            # count euclidian distance for each side of the rectangle\n",
    "            sideOne = np.linalg.norm(box[0]-box[1])\n",
    "            sideTwo = np.linalg.norm(box[0]-box[3])\n",
    "            \n",
    "            # count area of the rectangle\n",
    "            area = sideOne*sideTwo\n",
    "            \n",
    "            # find the largest rectangle within all contours\n",
    "            if area > largestArea:\n",
    "                largestArea = area\n",
    "                largestRect = box\n",
    "        \n",
    "\n",
    "    # draw contour of the found rectangle on  the original image\n",
    "    if largestArea > frameArea*0.02:\n",
    "        cv2.drawContours(frame,[largestRect],0,(0,0,255),2)\n",
    "        \n",
    "\n",
    "    cropped = None\n",
    "\n",
    "    if largestRect is not None:\n",
    "        # cropped interesting area\n",
    "        cropped = four_point_transform(frame, [largestRect][0])\n",
    "        \n",
    "    return frame, cropped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Function <font color = 'blue'>  <b>main</b></font>\n",
    "### <font color = #17243D>This function will create two windows. The first window displays what the camera sees.<br> \n",
    "<img src = \"JN Images\\camera2.jpg\"  style=\"width: 300px;\" > </h2>\n",
    "   \n",
    " \n",
    "### <font color = #17243D>The second window shows the image that was cropped around the blob.<br> \n",
    "<img src = \"JN Images\\cropped 2.jpg\"  style=\"width: 150px;\"> \n",
    "\n",
    "## Gathering Data\n",
    "<font color = #17243D>1. Run the program. <br> \n",
    "<font color = \"green\">2. Place an image of the \"green man\" in front of the camera. <br>\n",
    "<font color = \"green\">3. Press \"S\" to save the image. The images will be saved to the same location as this Jupyter Notebook file. <br>\n",
    "<font color = \"green\">4. Gather at least 250 images.<br>\n",
    "<font color = \"green\">5. Move the images to the folder testAI/crosswalk/Data/walk <br> \n",
    "<img src = \"JN Images\\green man.jpg\"  style=\"width: 200px;\" align = left> <br><br> <br> <br> <br> <br>  <br> <br>\n",
    "    \n",
    "<font color = \"red\">6. Gather at least 250 images of the red hand. <br>\n",
    "<font color = \"red\">7. Move the images to the folder testAI/crosswalk/Data/stop <br>\n",
    "<img src = \"JN Images\\red hand.jpg\"  style=\"width: 200px;\" align = left>\n",
    "    <br><br> <br> <br> <br> <br>  <br> <br> <br>\n",
    "\n",
    "\n",
    "<font color = \"black\">8. Gather 250 images for a set called \"none\". These images should be a black image without the red hand or green man. By having a \"none\" set, we will reduce the number of false positives. The image should look like the ones below.<br>\n",
    "<font color = \"black\">9. Move the images to the folder testAI/crosswalk/Data/none <br>\n",
    "<img src = \"JN Images\\none (1).jpg\"  style=\"height: 150px;\" align = left>\n",
    "<img src = \"JN Images\\none (2).jpg\"  style=\"height: 150px;\" align = left>\n",
    "<img src = \"JN Images\\none (3).jpg\"  style=\"height: 150px;\" align = left>\n",
    "<img src = \"JN Images\\none (4).jpg\"  style=\"height: 150px;\" align = left>  <br><br><br><br><br><br><br><br><br>\n",
    "    \n",
    "<font color = #17243D>10. Create a new folder named \"Test\". You should have 4 folders now. <br> \n",
    "<font color = #17243D>11. Move 50 pictures from the \"walk\", \"stop\", and \"none\" files to test. <font color = \"red\">Do not copy and paste. They must be different files.<br>\n",
    "<font color = #17243D>12. Select all of the photos of the green man. Press F2. Rename them \"walk\".<br>\n",
    "<font color = #17243D>13. Change the red hand to \"stop\". Change the rest to \"none\". <br>\n",
    "    \n",
    "<img src = \"JN Images\\test.jpg\"  style=\"height: 175px;\" align = left>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5c088c8b9f1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-5c088c8b9f1c>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# ------------------------------------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"win32\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsvcrt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------------------\n",
    "# Function: Main Program\n",
    "# Inputs:   \n",
    "# return:   \n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "def main():\n",
    "    if sys.platform == \"win32\":\n",
    "        import os, msvcrt\n",
    "        \n",
    "    # ------------------------------------------------------------------------------------------------------\n",
    "    # Main Program\n",
    "    # ------------------------------------------------------------------------------------------------------\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    \n",
    "    while(True):\n",
    "        # Capture the frames\n",
    "        grabbed, frame = video_capture.read()\n",
    "        \n",
    "        frame, cropped = findTrafficSign(grabbed, frame)\n",
    "                \n",
    "        #Display the resulting frame\n",
    "        cv2.imshow('frame',frame)\n",
    "\n",
    "        if cropped is not None:\n",
    "            cv2.imshow(\"crop\", cropped)\n",
    "\n",
    "        # Handle user keyboard inputs\n",
    "        key = cv2.waitKey(1);\n",
    "        \n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('s'):\n",
    "            if cropped is not None:\n",
    "                file = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S%f\") +'.jpg'\n",
    "                cv2.imwrite(file,cropped)\n",
    "                print(file, ' saved')\n",
    "\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finishing up\n",
    "\n",
    "You should have four folders in your Data folder:\n",
    "* walk\n",
    "* stop\n",
    "* none\n",
    "* test\n",
    "\n",
    "Each folder should have around 200 photos in them. Remember: the higher quality data that goes into the model, the better our results. \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<hr style = \"border-width: 10px;\" color = \"red\">\n",
    "<hr style = \"border-width: 10px;\" color = \"red\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'blue' > [ Data Exploration ] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "####  <font color = #17243D> Now, let's go through the \"walk\" and \" stop\"  folders and delete any low quality images. Here are some examples. <br>\n",
    "<img src = \"JN Images\\low quality (9).jpg\"  style=\"height: 200px;\" align = left> \n",
    "<br><br><br><br><br><br><br><br><br><br>\n",
    "<img src = \"JN Images\\low quality (19).jpg\"  style=\"height: 200px;\" align = left>  \n",
    " <br> <br> <br> <br> <br> <br> <br> <br> <br> <br>\n",
    "<img src = \"JN Images\\low quality (5).jpg\"  style=\"height: 200px;\" align = left>  \n",
    " <br> <br> <br> <br> <br> <br> <br> <br> <br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><hr style = \"border-width: 10px;\" color = \"red\"><hr style = \"border-width: 10px;\" color = \"red\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'blue' > [ Modeling ] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train a AI model to recognize the walk signal and the stop signal!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are the modules needed to create the model. Keras is the program that will create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, MaxPool2D, Conv2D, Flatten, Dropout, Input, BatchNormalization, Add\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These <font color = \"blue\"> functions </font> are used to run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worker function for custom model\n",
    "def conv_block(x, filters):\n",
    "    x = BatchNormalization() (x)\n",
    "    x = Conv2D(filters, (3, 3), activation='relu', padding='same') (x)\n",
    "\n",
    "    x = BatchNormalization() (x)\n",
    "    shortcut = x\n",
    "    x = Conv2D(filters, (3, 3), activation='relu', padding='same') (x)\n",
    "    x = Add() ([x, shortcut])\n",
    "    x = MaxPool2D((2, 2), strides=(2, 2)) (x)\n",
    "\n",
    "    return x\n",
    "\n",
    "# DIY model for training (instead of using standard model package)\n",
    "def custom_model(input_shape, n_classes):\n",
    "\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "\n",
    "    x = conv_block(input_tensor, 32)\n",
    "    x = conv_block(x, 64)\n",
    "    x = conv_block(x, 128)\n",
    "    x = conv_block(x, 256)\n",
    "    x = conv_block(x, 512)\n",
    "\n",
    "    x = Flatten() (x)\n",
    "    x = BatchNormalization() (x)\n",
    "    x = Dense(512, activation='relu') (x)\n",
    "    x = Dense(512, activation='relu') (x)\n",
    "\n",
    "    output_layer = Dense(n_classes, activation='softmax') (x)\n",
    "\n",
    "    inputs = [input_tensor]\n",
    "    model = Model(inputs, output_layer)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Below is the main loop. Let's look the important parts. \n",
    "\n",
    "#### This code finds the image in the folders. <font color = \"red\"> Make sure that the file path is completely correct. If it is wrong, the model won't work. </font>\n",
    "\n",
    "    data_dir = 'C:\\testAI\\CV'    \n",
    "    match_obj1 = os.path.join('c:\\\\', 'testAI', 'CV', 'Crosswalk', 'Data', 'walk', '*.jpg')\n",
    "    paths_obj1 = glob.glob(match_obj1)\n",
    "    \n",
    "    match_obj2 = os.path.join('c:\\\\', 'testAI', 'CV', 'Crosswalk', 'Data','stop', '*.jpg')\n",
    "    paths_obj2 = glob.glob(match_obj2)\n",
    "\n",
    "    match_obj3 = os.path.join('c:\\\\', 'testAI', 'CV',  'Crosswalk', 'Data','none', '*.jpg')\n",
    "    paths_obj3 = glob.glob(match_obj3)\n",
    "\n",
    "    match_test = os.path.join('c:\\\\', 'testAI', 'CV',  'Crosswalk', 'Data','Test', '*.jpg')\n",
    "    paths_test = glob.glob(match_test) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "When we run the code, it will create a model that knows the difference between walk signal and a stop signal. The code will out put two new files into the \"Crosswalk\" folder: <font color = \"blue\"> model.json </font> and <font color = \"blue\"> weights.h5 </font>. The code will take a few minutes to run.  <br>\n",
    "    \n",
    "<img src = \"JN images/model.jpg\" style = \"height: 200px;\" >\n",
    "<br><br> After training, the model will test itself against all the pictures in the <b>test</b> folder. Check to see if the results match. Look at the table below. Can you find a problem?\n",
    "\n",
    "\n",
    "|File Name | Forecast Category |\n",
    "| :- | :-: |\n",
    "|walk (1).jpg | walk|\n",
    "|walk (2).jpg | walk|\n",
    "|walk (3).jpg | walk|\n",
    "|stop (1).jpg | stop |\n",
    "|stop (2).jpg | <font color = \"red\"> walk </font> |\n",
    "|stop (3).jpg | <font color = \"red\"> none</font>  |\n",
    "|none (1).jpg | none |\n",
    "|none (2).jpg | none |\n",
    "|none (3).jpg | none |\n",
    "<br>\n",
    "\n",
    "How can we fix this problem?<br>\n",
    "* Solution 1: Check all the collected data and make sure there are no low quality pictures.\n",
    "* Solution 2: Add more pictures to the \"walk\", \"stop\", and \"none\" folders. The model will be more accurate with more pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 668 samples, validate on 167 samples\n",
      "Epoch 1/8\n",
      "668/668 [==============================] - 34s 50ms/step - loss: 0.3036 - acc: 0.9147 - val_loss: 1.8452e-05 - val_acc: 1.0000\n",
      "Epoch 2/8\n",
      "668/668 [==============================] - 19s 28ms/step - loss: 0.0213 - acc: 0.9955 - val_loss: 7.2260e-04 - val_acc: 1.0000\n",
      "Epoch 3/8\n",
      "668/668 [==============================] - 19s 28ms/step - loss: 0.0304 - acc: 0.9910 - val_loss: 3.7580e-04 - val_acc: 1.0000\n",
      "Epoch 4/8\n",
      "668/668 [==============================] - 19s 28ms/step - loss: 0.0058 - acc: 0.9985 - val_loss: 5.8373e-04 - val_acc: 1.0000\n",
      "Epoch 5/8\n",
      "668/668 [==============================] - 20s 30ms/step - loss: 0.0036 - acc: 0.9985 - val_loss: 2.3949e-07 - val_acc: 1.0000\n",
      "Epoch 6/8\n",
      "668/668 [==============================] - 19s 28ms/step - loss: 0.0128 - acc: 0.9985 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 7/8\n",
      "668/668 [==============================] - 19s 28ms/step - loss: 5.9160e-04 - acc: 1.0000 - val_loss: 8.6306e-04 - val_acc: 1.0000\n",
      "Epoch 8/8\n",
      "668/668 [==============================] - 19s 28ms/step - loss: 1.3763e-04 - acc: 1.0000 - val_loss: 5.1193e-05 - val_acc: 1.0000\n",
      "File name\t forecast category\n",
      "none (10).jpg\tnone\n",
      "none (11).jpg\tnone\n",
      "none (12).jpg\tnone\n",
      "none (13).jpg\tnone\n",
      "none (14).jpg\tnone\n",
      "none (15).jpg\tnone\n",
      "none (16).jpg\tnone\n",
      "none (17).jpg\tnone\n",
      "none (18).jpg\tnone\n",
      "none (19).jpg\tnone\n",
      "none (2).jpg\tnone\n",
      "none (20).jpg\tnone\n",
      "none (21).jpg\tnone\n",
      "none (22).jpg\tnone\n",
      "none (23).jpg\tnone\n",
      "none (24).jpg\tnone\n",
      "none (25).jpg\tnone\n",
      "none (26).jpg\tnone\n",
      "none (27).jpg\tnone\n",
      "none (28).jpg\tnone\n",
      "none (29).jpg\tnone\n",
      "none (3).jpg\tnone\n",
      "none (30).jpg\tnone\n",
      "none (31).jpg\tnone\n",
      "none (32).jpg\tnone\n",
      "none (33).jpg\tnone\n",
      "none (34).jpg\tnone\n",
      "none (35).jpg\tnone\n",
      "none (36).jpg\tnone\n",
      "none (37).jpg\tnone\n",
      "none (38).jpg\tnone\n",
      "none (39).jpg\tnone\n",
      "none (4).jpg\tnone\n",
      "none (40).jpg\tnone\n",
      "none (41).jpg\tnone\n",
      "none (42).jpg\tnone\n",
      "none (43).jpg\tnone\n",
      "none (44).jpg\tnone\n",
      "none (45).jpg\tnone\n",
      "none (46).jpg\tnone\n",
      "none (5).jpg\tnone\n",
      "none (6).jpg\tnone\n",
      "none (7).jpg\tnone\n",
      "none (8).jpg\tnone\n",
      "none (9).jpg\tnone\n",
      "stop (1).jpg\tstop\n",
      "stop (10).jpg\tstop\n",
      "stop (11).jpg\tstop\n",
      "stop (12).jpg\tstop\n",
      "stop (13).jpg\tstop\n",
      "stop (14).jpg\tstop\n",
      "stop (15).jpg\tstop\n",
      "stop (16).jpg\tstop\n",
      "stop (17).jpg\tstop\n",
      "stop (18).jpg\tstop\n",
      "stop (19).jpg\tstop\n",
      "stop (2).jpg\tstop\n",
      "stop (20).jpg\tstop\n",
      "stop (21).jpg\tstop\n",
      "stop (22).jpg\tstop\n",
      "stop (23).jpg\tstop\n",
      "stop (24).jpg\tstop\n",
      "stop (25).jpg\tstop\n",
      "stop (26).jpg\tstop\n",
      "stop (27).jpg\tstop\n",
      "stop (28).jpg\tstop\n",
      "stop (29).jpg\tstop\n",
      "stop (3).jpg\tstop\n",
      "stop (30).jpg\tstop\n",
      "stop (31).jpg\tstop\n",
      "stop (32).jpg\tstop\n",
      "stop (33).jpg\tstop\n",
      "stop (34).jpg\tstop\n",
      "stop (35).jpg\tstop\n",
      "stop (36).jpg\tstop\n",
      "stop (37).jpg\tstop\n",
      "stop (38).jpg\tstop\n",
      "stop (39).jpg\tstop\n",
      "stop (4).jpg\tstop\n",
      "stop (40).jpg\tstop\n",
      "stop (41).jpg\tstop\n",
      "stop (42).jpg\tstop\n",
      "stop (43).jpg\tstop\n",
      "stop (44).jpg\tstop\n",
      "stop (45).jpg\tstop\n",
      "stop (46).jpg\tstop\n",
      "stop (47).jpg\tstop\n",
      "stop (48).jpg\tstop\n",
      "stop (49).jpg\tstop\n",
      "stop (5).jpg\tstop\n",
      "stop (50).jpg\tstop\n",
      "stop (51).jpg\tstop\n",
      "stop (52).jpg\tstop\n",
      "stop (53).jpg\tstop\n",
      "stop (54).jpg\tstop\n",
      "stop (55).jpg\tstop\n",
      "stop (56).jpg\tstop\n",
      "stop (6).jpg\tstop\n",
      "stop (7).jpg\tstop\n",
      "stop (8).jpg\tstop\n",
      "stop (9).jpg\tstop\n",
      "walk (1).jpg\twalk\n",
      "walk (10).jpg\twalk\n",
      "walk (11).jpg\twalk\n",
      "walk (12).jpg\twalk\n",
      "walk (13).jpg\twalk\n",
      "walk (14).jpg\twalk\n",
      "walk (15).jpg\twalk\n",
      "walk (16).jpg\twalk\n",
      "walk (17).jpg\twalk\n",
      "walk (18).jpg\twalk\n",
      "walk (19).jpg\twalk\n",
      "walk (2).jpg\twalk\n",
      "walk (20).jpg\twalk\n",
      "walk (21).jpg\twalk\n",
      "walk (22).jpg\twalk\n",
      "walk (23).jpg\twalk\n",
      "walk (24).jpg\twalk\n",
      "walk (25).jpg\twalk\n",
      "walk (26).jpg\twalk\n",
      "walk (27).jpg\twalk\n",
      "walk (28).jpg\twalk\n",
      "walk (29).jpg\twalk\n",
      "walk (3).jpg\twalk\n",
      "walk (30).jpg\twalk\n",
      "walk (31).jpg\twalk\n",
      "walk (32).jpg\twalk\n",
      "walk (33).jpg\twalk\n",
      "walk (34).jpg\twalk\n",
      "walk (35).jpg\twalk\n",
      "walk (4).jpg\twalk\n",
      "walk (5).jpg\twalk\n",
      "walk (6).jpg\twalk\n",
      "walk (7).jpg\twalk\n",
      "walk (8).jpg\twalk\n",
      "walk (9).jpg\twalk\n"
     ]
    }
   ],
   "source": [
    "# main loop\n",
    "def main():\n",
    "          \n",
    "    # Data parameter\n",
    "    input_height = 48\n",
    "    input_width = 48\n",
    "    \n",
    "    input_channel = 3\n",
    "    input_shape = (input_height, input_width, input_channel)\n",
    "    n_classes = 4 # 4 objects\n",
    "\n",
    "    # Modeling\n",
    "    # 'custom':\n",
    "    model = custom_model(input_shape, n_classes)\n",
    "  \n",
    "    adam = Adam()\n",
    "    model.compile(\n",
    "        optimizer=adam,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['acc'],\n",
    "    )\n",
    "\n",
    "    # Search all images\n",
    "    data_dir = 'C:\\testAI\\CV'    \n",
    "    match_obj1 = os.path.join('c:\\\\', 'testAI','Crosswalk', 'Data', 'walk', '*.jpg')\n",
    "    paths_obj1 = glob.glob(match_obj1)\n",
    "    \n",
    "    match_obj2 = os.path.join('c:\\\\', 'testAI', 'Crosswalk', 'Data','stop', '*.jpg')\n",
    "    paths_obj2 = glob.glob(match_obj2)\n",
    "\n",
    "    match_obj3 = os.path.join('c:\\\\', 'testAI','Crosswalk', 'Data','none', '*.jpg')\n",
    "    paths_obj3 = glob.glob(match_obj3)\n",
    "\n",
    "    match_test = os.path.join('c:\\\\', 'testAI', 'Crosswalk', 'Data','Test', '*.jpg')\n",
    "    paths_test = glob.glob(match_test)\n",
    "\n",
    "    n_train = len(paths_obj1) + len(paths_obj2) + len(paths_obj3) \n",
    "    n_test = len(paths_test)\n",
    "\n",
    "    # Initialization dataset matrix\n",
    "    trainset = np.zeros(\n",
    "        shape=(n_train, input_height, input_width, input_channel),\n",
    "        dtype='float32',\n",
    "    )\n",
    "    label = np.zeros(\n",
    "        shape=(n_train, n_classes),\n",
    "        dtype='float32',\n",
    "    )\n",
    "    testset = np.zeros(\n",
    "        shape=(n_test, input_height, input_width, input_channel),\n",
    "        dtype='float32',\n",
    "    )\n",
    "\n",
    "    # Read image and resize to data set\n",
    "    paths_train = paths_obj1 + paths_obj2 + paths_obj3 \n",
    "\n",
    "    for ind, path in enumerate(paths_train):      \n",
    "        try:\n",
    "            image = cv2.imread(path)\n",
    "            resized_image = cv2.resize(image, (input_width, input_height))\n",
    "            trainset[ind] = resized_image\n",
    "\n",
    "        except Exception as e:\n",
    "            print(path) # print out the Image that cause exception error\n",
    "        \n",
    "    for ind, path in enumerate(paths_test):\n",
    "        try:\n",
    "            image = cv2.imread(path)\n",
    "            resized_image = cv2.resize(image, (input_width, input_height))\n",
    "            testset[ind] = resized_image\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(path) # print out the Image that cause exception error\n",
    "\n",
    "    \n",
    "    n_obj1 = len(paths_obj1)\n",
    "    n_obj2 = len(paths_obj2)\n",
    "    n_obj3 = len(paths_obj3)\n",
    "    \n",
    "    begin_ind = 0\n",
    "    end_ind = n_obj1\n",
    "    label[begin_ind:end_ind, 0] = 1.0\n",
    "\n",
    "    begin_ind = n_obj1\n",
    "    end_ind = n_obj1 + n_obj2\n",
    "    label[begin_ind:end_ind, 1] = 1.0\n",
    "   \n",
    "    begin_ind = n_obj1 + n_obj2\n",
    "    end_ind = n_obj1 + n_obj2 + n_obj3\n",
    "    label[begin_ind:end_ind, 2] = 1.0\n",
    "    \n",
    "    \n",
    "    # Normalize the value between 0 and 1\n",
    "    trainset = trainset / 255.0\n",
    "    testset = testset / 255.0\n",
    "\n",
    "    # Training model\n",
    "    model.fit(\n",
    "        trainset,\n",
    "        label,    \n",
    "        epochs=8,  # no. of rounds of training => 8 rounds\n",
    "        validation_split=0.2,   # percentage of dataset use for validation at trainiing => 20% (2000 images, 1600 for training, 400 for validation)\n",
    "    )\n",
    "\n",
    "    # Saving model architecture and weights (parameters)\n",
    "    model_desc = model.to_json()\n",
    "    model_file = 'C:/testAI/Crosswalk/model.json'\n",
    "    with open(model_file, 'w') as file_model:                           \n",
    "        file_model.write(model_desc)\n",
    "\n",
    "    weights_file = 'C:/testAI/Crosswalk/weights.h5'\n",
    "    model.save_weights(weights_file )\n",
    "\n",
    "    # Execution predication\n",
    "    if testset.shape[0] != 0:\n",
    "        result_onehot = model.predict(testset)\n",
    "        result_sparse = np.argmax(result_onehot, axis=1)\n",
    "    else:\n",
    "        result_sparse = list()\n",
    "    \n",
    "    # Print predication results\n",
    "    print('File name\\t forecast category')\n",
    "\n",
    "    for path, label_id in zip(paths_test, result_sparse):\n",
    "      filename = os.path.basename(path)\n",
    "      if label_id == 0:\n",
    "          label_name = 'walk'\n",
    "      elif label_id == 1:\n",
    "          label_name = 'stop'\n",
    "      elif label_id == 2:\n",
    "          label_name = 'none'\n",
    "     \n",
    "     \n",
    "      print('%s\\t%s' % (filename, label_name))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> <br> <hr style = \"border-width: 10px;\" color = \"red\"><hr style = \"border-width: 10px;\" color = \"red\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'blue' > [ Test the Model ] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how well our model works. Here is a video of the program working. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"JN Images/Test the Model.mp4\" style=\"width:400px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the modules that we need to import. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b4b9342c5672>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mserial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mimutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperspective\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfour_point_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imutils'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import serial\n",
    "from imutils.perspective import four_point_transform\n",
    "\n",
    "import imutils\n",
    "from keras.models import model_from_json\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Do you recognize this function? It's same code from above. This function finds the traffic sign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------\n",
    "# Function: To detect Traffic Sign\n",
    "# Inputs:   \n",
    "# return:   \n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "def findTrafficSign(grabbed, frame):\n",
    "    '''\n",
    "    This function find blobs with black color on the image.\n",
    "    After blobs were found it detects the largest square blob, that must be the sign.\n",
    "    '''\n",
    "    # define range HSV for  color of the traffic sign\n",
    "    \n",
    "    lower_color = np.array([0,0,0])\n",
    "    upper_color = np.array([179,255,60])\n",
    "\n",
    "    if not grabbed:\n",
    "        print(\"No input image\")\n",
    "        return\n",
    "    \n",
    "    # frame = imutils.resize(frame, width=500)\n",
    "    frameArea = frame.shape[0]*frame.shape[1]\n",
    "    \n",
    "    # convert color image to HSV color scheme\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # define kernel for smoothing   \n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    # extract binary image with active blue regions\n",
    "    mask = cv2.inRange(hsv, lower_color, upper_color)\n",
    "    # morphological operations\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # find contours in the mask\n",
    "    cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]\n",
    "       \n",
    "    # define variables to hold values during loop\n",
    "    largestArea = 0\n",
    "    largestRect = None\n",
    "    \n",
    "    # only proceed if at least one contour was found\n",
    "    if len(cnts) > 0:\n",
    "        for cnt in cnts:\n",
    "            rect = cv2.minAreaRect(cnt)\n",
    "            box = cv2.boxPoints(rect)\n",
    "            box = np.int0(box)\n",
    "            \n",
    "            # count euclidian distance for each side of the rectangle\n",
    "            sideOne = np.linalg.norm(box[0]-box[1])\n",
    "            sideTwo = np.linalg.norm(box[0]-box[3])\n",
    "            # count area of the rectangle\n",
    "            area = sideOne*sideTwo\n",
    "            # find the largest rectangle within all contours\n",
    "            if area > largestArea:\n",
    "                largestArea = area\n",
    "                largestRect = box\n",
    "        \n",
    "\n",
    "    # draw contour of the found rectangle on  the original image\n",
    "    if largestArea > frameArea*0.02:    # 0.02\n",
    "        cv2.drawContours(frame,[largestRect],0,(0,0,255),2)\n",
    "        \n",
    "    cropped = None\n",
    "    if largestRect is not None:\n",
    "        # cropped interesting area\n",
    "        cropped = four_point_transform(frame, [largestRect][0])\n",
    "        \n",
    "    return frame, cropped, largestRect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>This is the function that guesses if the traffic sign is a walk sign, stop sign, or a nothing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------\n",
    "# Function: To read Traffic Sign\n",
    "# Inputs:   \n",
    "# return:   \n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "def ReadTrafficSign(orig_image, model):\n",
    "    ret = -1\n",
    "    \n",
    "    # Check if the stream is over\n",
    "    if orig_image is None:\n",
    "        return ret\n",
    "\n",
    "    # Scale to the dimension entered by the model, adjust the value range from 0 to 1.\n",
    "    input_width = 48\n",
    "    input_height = 48\n",
    "    resized_image = cv2.resize(\n",
    "        orig_image,\n",
    "        (input_width, input_height),\n",
    "    ).astype(np.float32)\n",
    "    normalized_image = resized_image / 255.0\n",
    "\n",
    "    # Execution forecast\n",
    "    batch = normalized_image.reshape(1, input_height, input_width, 3)\n",
    "    result_onehot = model.predict(batch)\n",
    "    obj1_score, obj2_score, obj3_score, obj4_score = result_onehot[0]\n",
    "    class_id = np.argmax(result_onehot, axis=1)[0]\n",
    "\n",
    "    if class_id == 0:\n",
    "        class_str = 'walk'\n",
    "        score = obj1_score\n",
    "        ret = 1\n",
    "    elif class_id == 1:\n",
    "        class_str = 'stop'\n",
    "        score = obj2_score\n",
    "        ret = 1\n",
    "    elif class_id == 2:\n",
    "        class_str = 'none'\n",
    "        score = obj3_score\n",
    "        ret = 1\n",
    "\n",
    "    return ret, class_str, score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br> \n",
    "Here is the main function. It is very similar to the code from the fire detection lesson. When you run the code, it will cycle between the <font color = \"blue\"> FindTraffic</font> and <font color = \"blue\"> ReadTrafficSign </font> functions. <br>\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press [q] to quit\n",
      "User quit program...\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------------------\n",
    "# Function: Main Program\n",
    "# Inputs:   \n",
    "# return:   \n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "def main():\n",
    "    # ------------------------------------------------------------------------------------------------------\n",
    "    # Global Variable intialization\n",
    "    # ------------------------------------------------------------------------------------------------------\n",
    "  \n",
    "    READ_SIGN = 2\n",
    "    mode_status = READ_SIGN     # mode to look for the traffic sign or to read it.\n",
    "    TIC = time.time()       # timeout reference\n",
    "\n",
    "\n",
    "     \n",
    "    # ------------------------------------------------------------------------------------------------------\n",
    "    # Main Program\n",
    "    # ------------------------------------------------------------------------------------------------------\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "    # Loading model\n",
    "    with open('C:/testAI/Crosswalk/model.json', 'r') as file_model:\n",
    "        model_desc = file_model.read()\n",
    "        model = model_from_json(model_desc)\n",
    "\n",
    "    model.load_weights('C:/testAI/Crosswalk/weights.h5')\n",
    "    print('Press [q] to quit')\n",
    "\n",
    "    while(True):\n",
    "        # Capture the frames\n",
    "        grabbed, frame = video_capture.read()\n",
    "        width = video_capture.get(cv2.CAP_PROP_FRAME_WIDTH )\n",
    "        height = video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT )\n",
    "        frame, cropped, largestRect = findTrafficSign(grabbed, frame)\n",
    "        \n",
    "           \n",
    "        if cropped is not None:\n",
    "            ret, detectedTrafficSign, score = ReadTrafficSign(cropped, model)\n",
    "            percent = score *100\n",
    "            printTrafficSign = detectedTrafficSign + ' ' + str('%.2f' % percent) + '%'             # 2f\n",
    "            cv2.putText(frame, printTrafficSign, (10,50), cv2.FONT_HERSHEY_SIMPLEX, 2, (100, 255, 100), 2)\n",
    "                                          \n",
    "                                             \n",
    "        #Display the resulting frame\n",
    "        cv2.imshow('frame', frame)\n",
    "\n",
    "        # Handle user keyboard inputs\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            print('User quit program...')\n",
    "            break\n",
    "        \n",
    "            \n",
    "   \n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<hr style = \"border-width: 10px;\" color = \"red\"><hr style = \"border-width: 10px;\" color = \"red\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'blue' > [ Deployment ] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to make the project. It's simple.\n",
    "\n",
    "#### Components\n",
    "* Micro:bit x 2\n",
    "* Expansion Board\n",
    "* Power Source\n",
    "* Buzzer\n",
    "* Wires\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the project\n",
    "\n",
    "* Attach the buzzer to P0\n",
    "* Attach the power source\n",
    "* Insert the micro:bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"JN images\\model.png\" style = \"height: 250px;\" align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Master Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"JN images\\master.png\" style = \"height: 250px;\" align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"JN images\\client.png\" style = \"height: 400px;\" align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's connect our AI model to the Micro:bit. The below final code uses the same <font color = \"blue\"> SerialSendCommand </font> function as the <font color = \"red\"> Fire Detection</font> project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Don't forget to change the COM # for your computer. <br>\n",
    "\n",
    "<code> ser = serial.Serial('COM58', 115200, timeout=0, parity=serial.PARITY_NONE, rtscts=0)</code>\n",
    "\n",
    "\n",
    "<br> The below code is changed. This means that the code will only serial write to the micro:bit on time every 5 seconds.<br>\n",
    "\n",
    " <code>  if time.time() - TIC > 5:</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIDEO\n",
    "\n",
    "Watch a video of the project!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"JN Images/Crosswalk.mp4\" style=\"width:500px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-20-484c6d5d3210>, line 179)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-20-484c6d5d3210>\"\u001b[1;36m, line \u001b[1;32m179\u001b[0m\n\u001b[1;33m    percent = score *100\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import serial\n",
    "from imutils.perspective import four_point_transform\n",
    "import imutils\n",
    "from keras.models import model_from_json\n",
    "import time\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# Function: to send command (string with new line) to micro:bit\n",
    "# Input: Cmd - the string command to send (one character)\n",
    "# Return: none\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "def SerialSendCommand(ser, Cmd):\n",
    "    Cmd_Str = Cmd + '\\n'\n",
    "    cmd_bytes = str.encode(Cmd_Str)\n",
    "    ser.write(cmd_bytes)\n",
    "    \n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# Function: To detect Traffic Sign\n",
    "# Inputs:   \n",
    "# return:   \n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "def findTrafficSign(grabbed, frame):\n",
    "\n",
    "\n",
    "    # define range HSV for  color of the traffic sign\n",
    "    \n",
    "    lower_color = np.array([0,0,0])\n",
    "    upper_color = np.array([179,255,60])\n",
    "\n",
    "    if not grabbed:\n",
    "        print(\"No input image\")\n",
    "        return\n",
    "    \n",
    "    # frame = imutils.resize(frame, width=500)\n",
    "    frameArea = frame.shape[0]*frame.shape[1]\n",
    "    \n",
    "    # convert color image to HSV color scheme\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # define kernel for smoothing   \n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    # extract binary image with active blue regions\n",
    "    mask = cv2.inRange(hsv, lower_color, upper_color)\n",
    "    # morphological operations\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # find contours in the mask\n",
    "    cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]\n",
    "    \n",
    "    # defite string variable to hold detected sign description\n",
    "    detectedTrafficSign = None\n",
    "    \n",
    "    # define variables to hold values during loop\n",
    "    largestArea = 0\n",
    "    largestRect = None\n",
    "    \n",
    "    # only proceed if at least one contour was found\n",
    "    if len(cnts) > 0:\n",
    "        for cnt in cnts:\n",
    "\n",
    "            # It is obtained by the function cv2.boxPoints()\n",
    "            rect = cv2.minAreaRect(cnt)\n",
    "            box = cv2.boxPoints(rect)\n",
    "            box = np.int0(box)\n",
    "            \n",
    "            # count euclidian distance for each side of the rectangle\n",
    "            sideOne = np.linalg.norm(box[0]-box[1])\n",
    "            sideTwo = np.linalg.norm(box[0]-box[3])\n",
    "            # count area of the rectangle\n",
    "            area = sideOne*sideTwo\n",
    "            # find the largest rectangle within all contours\n",
    "            if area > largestArea:\n",
    "                largestArea = area\n",
    "                largestRect = box\n",
    "        \n",
    "\n",
    "    # draw contour of the found rectangle on  the original image\n",
    "    if largestArea > frameArea*0.02:    # 0.02\n",
    "        cv2.drawContours(frame,[largestRect],0,(0,0,255),2)\n",
    "        \n",
    "    cropped = None\n",
    "    if largestRect is not None:\n",
    "        # cropped interesting area\n",
    "        cropped = four_point_transform(frame, [largestRect][0])\n",
    "        \n",
    "        \n",
    "\n",
    "    return frame, cropped, largestRect\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# Function: To read Traffic Sign\n",
    "# Inputs:   \n",
    "# return:   \n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "def ReadTrafficSign(orig_image, model):\n",
    "    ret = -1\n",
    "    \n",
    "    # Check if the stream is over\n",
    "    if orig_image is None:\n",
    "        return ret\n",
    "\n",
    "    # Scale to the dimension entered by the model, adjust the value range from 0 to 1.\n",
    "    input_width = 48\n",
    "    input_height = 48\n",
    "    resized_image = cv2.resize(\n",
    "        orig_image,\n",
    "        (input_width, input_height),\n",
    "    ).astype(np.float32)\n",
    "    normalized_image = resized_image / 255.0\n",
    "\n",
    "    # Execution forecast\n",
    "    batch = normalized_image.reshape(1, input_height, input_width, 3)\n",
    "    result_onehot = model.predict(batch)\n",
    "    obj1_score, obj2_score, obj3_score, obj4_score = result_onehot[0]\n",
    "    class_id = np.argmax(result_onehot, axis=1)[0]\n",
    "\n",
    "    if class_id == 0:\n",
    "        class_str = 'walk'\n",
    "        score = obj1_score\n",
    "        ret = 1\n",
    "    elif class_id == 1:\n",
    "        class_str = 'stop'\n",
    "        score = obj2_score\n",
    "        ret = 1\n",
    "    elif class_id == 2:\n",
    "        class_str = 'none'\n",
    "        score = obj3_score\n",
    "        ret = 1\n",
    "\n",
    "    return ret, class_str, score\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# Function: Main Program\n",
    "# Inputs:   \n",
    "# return:   \n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "def main():\n",
    "    # ------------------------------------------------------------------------------------------------------\n",
    "    # Global Variable intialization\n",
    "    # ------------------------------------------------------------------------------------------------------\n",
    "    FIND_BLACK = 0\n",
    "    READ_SIGN = 2\n",
    "    mode_status = READ_SIGN     # mode to indicate line tracing, inference or junction maneuver\n",
    "    TIC = 6       # timeout reference\n",
    "\n",
    "\n",
    "\n",
    "    # Opening serial port COM25, baud 115200, no parity, no flow control\n",
    "    ser = serial.Serial('COM22', 115200, timeout=0, parity=serial.PARITY_NONE, rtscts=0)\n",
    "    \n",
    "    # ------------------------------------------------------------------------------------------------------\n",
    "    # Main Program\n",
    "    # ------------------------------------------------------------------------------------------------------\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "    # Loading model\n",
    "    with open('C:/testAI/Crosswalk/model.json', 'r') as file_model:\n",
    "        model_desc = file_model.read()\n",
    "        model = model_from_json(model_desc)\n",
    "\n",
    "    model.load_weights('C:/testAI/Crosswalk/weights.h5')\n",
    "    print('Press [q] to quit.')\n",
    "\n",
    "    while(True):\n",
    "        # Capture the frames\n",
    "        grabbed, frame = video_capture.read()\n",
    "        width = video_capture.get(cv2.CAP_PROP_FRAME_WIDTH )\n",
    "        height = video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT )\n",
    "        frame, cropped, largestRect = findTrafficSign(grabbed, frame)\n",
    "        \n",
    "        ret, detectedTrafficSign, score = ReadTrafficSign(cropped, model)\n",
    "        #if ret == 1:\n",
    "        percent = score *100\n",
    "        printTrafficSign = detectedTrafficSign + ' ' + str('%.2f' % percent) + '%'\n",
    "        cv2.putText(frame, printTrafficSign, tuple(largestRect[0]), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 255, 0), 2)\n",
    "        \n",
    "        if time.time() - TIC > 5:           \n",
    "                \n",
    "            if cropped is not None:\n",
    "                 \n",
    "                if score > 0.99:\n",
    "                    if 'walk' in detectedTrafficSign:\n",
    "                        SerialSendCommand(ser, \"w\")\n",
    "                        TIC = time.time()\n",
    "                        print(\"walk\")\n",
    "\n",
    "                    elif 'stop' in detectedTrafficSign:\n",
    "                        SerialSendCommand(ser, \"s\")\n",
    "                        TIC = time.time()\n",
    "                        print(\"stop\")\n",
    "\n",
    "                    elif 'none' in detectedTrafficSign:\n",
    "                        pass \n",
    "                                                        \n",
    "                                                \n",
    "        #Display the resulting frame\n",
    "        cv2.imshow('frame', frame)\n",
    "\n",
    "        # Handle user keyboard inputs\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            print('User quit program...')\n",
    "            break\n",
    "        \n",
    "            \n",
    "    \n",
    "    ser.close()\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press [q] to quit\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import serial\n",
    "from imutils.perspective import four_point_transform\n",
    "import imutils\n",
    "from keras.models import model_from_json\n",
    "import time\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# Function: to send command (string with new line) to micro:bit\n",
    "# Input: Cmd - the string command to send (one character)\n",
    "# Return: none\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "def SerialSendCommand(ser, Cmd):\n",
    "    Cmd_Str = Cmd + '\\n'\n",
    "    cmd_bytes = str.encode(Cmd_Str)\n",
    "    ser.write(cmd_bytes)\n",
    "    \n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# Function: To detect Traffic Sign\n",
    "# Inputs:   \n",
    "# return:   \n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "def findTrafficSign(grabbed, frame):\n",
    "\n",
    "\n",
    "    # define range HSV for  color of the traffic sign\n",
    "    \n",
    "    lower_color = np.array([0,0,0])\n",
    "    upper_color = np.array([179,255,60])\n",
    "\n",
    "    if not grabbed:\n",
    "        print(\"No input image\")\n",
    "        return\n",
    "    \n",
    "    # frame = imutils.resize(frame, width=500)\n",
    "    frameArea = frame.shape[0]*frame.shape[1]\n",
    "    \n",
    "    # convert color image to HSV color scheme\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # define kernel for smoothing   \n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    # extract binary image with active blue regions\n",
    "    mask = cv2.inRange(hsv, lower_color, upper_color)\n",
    "    # morphological operations\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # find contours in the mask\n",
    "    cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]\n",
    "    \n",
    "    # defite string variable to hold detected sign description\n",
    "    detectedTrafficSign = None\n",
    "    \n",
    "    # define variables to hold values during loop\n",
    "    largestArea = 0\n",
    "    largestRect = None\n",
    "    \n",
    "    # only proceed if at least one contour was found\n",
    "    if len(cnts) > 0:\n",
    "        for cnt in cnts:\n",
    "\n",
    "            # It is obtained by the function cv2.boxPoints()\n",
    "            rect = cv2.minAreaRect(cnt)\n",
    "            box = cv2.boxPoints(rect)\n",
    "            box = np.int0(box)\n",
    "            \n",
    "            # count euclidian distance for each side of the rectangle\n",
    "            sideOne = np.linalg.norm(box[0]-box[1])\n",
    "            sideTwo = np.linalg.norm(box[0]-box[3])\n",
    "            # count area of the rectangle\n",
    "            area = sideOne*sideTwo\n",
    "            # find the largest rectangle within all contours\n",
    "            if area > largestArea:\n",
    "                largestArea = area\n",
    "                largestRect = box\n",
    "        \n",
    "\n",
    "    # draw contour of the found rectangle on  the original image\n",
    "    if largestArea > frameArea*0.02:    # 0.02\n",
    "        cv2.drawContours(frame,[largestRect],0,(0,0,255),2)\n",
    "        \n",
    "    cropped = None\n",
    "    if largestRect is not None:\n",
    "        # cropped interesting area\n",
    "        cropped = four_point_transform(frame, [largestRect][0])\n",
    "        \n",
    "        \n",
    "\n",
    "    return frame, cropped, largestRect\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# Function: To read Traffic Sign\n",
    "# Inputs:   \n",
    "# return:   \n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "def ReadTrafficSign(orig_image, model):\n",
    "    ret = -1\n",
    "    \n",
    "    # Check if the stream is over\n",
    "    if orig_image is None:\n",
    "        return ret\n",
    "\n",
    "    # Scale to the dimension entered by the model, adjust the value range from 0 to 1.\n",
    "    input_width = 48\n",
    "    input_height = 48\n",
    "    resized_image = cv2.resize(\n",
    "        orig_image,\n",
    "        (input_width, input_height),\n",
    "    ).astype(np.float32)\n",
    "    normalized_image = resized_image / 255.0\n",
    "\n",
    "    # Execution forecast\n",
    "    batch = normalized_image.reshape(1, input_height, input_width, 3)\n",
    "    result_onehot = model.predict(batch)\n",
    "    obj1_score, obj2_score, obj3_score, obj4_score = result_onehot[0]\n",
    "    class_id = np.argmax(result_onehot, axis=1)[0]\n",
    "\n",
    "    if class_id == 0:\n",
    "        class_str = 'walk'\n",
    "        score = obj1_score\n",
    "        ret = 1\n",
    "    elif class_id == 1:\n",
    "        class_str = 'stop'\n",
    "        score = obj2_score\n",
    "        ret = 1\n",
    "    elif class_id == 2:\n",
    "        class_str = 'none'\n",
    "        score = obj3_score\n",
    "        ret = 1\n",
    "\n",
    "    return ret, class_str, score\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# Function: Main Program\n",
    "# Inputs:   \n",
    "# return:   \n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "def main():\n",
    "    # ------------------------------------------------------------------------------------------------------\n",
    "    # Global Variable intialization\n",
    "    # ------------------------------------------------------------------------------------------------------\n",
    "  \n",
    "    READ_SIGN = 2\n",
    "    mode_status = READ_SIGN     # mode to look for the traffic sign or to read it.\n",
    "    TIC = time.time()       # timeout reference\n",
    "\n",
    "\n",
    "     \n",
    "    # ------------------------------------------------------------------------------------------------------\n",
    "    # Main Program\n",
    "    # ------------------------------------------------------------------------------------------------------\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "    # Loading model\n",
    "    with open('C:/testAI/Crosswalk/model.json', 'r') as file_model:\n",
    "        model_desc = file_model.read()\n",
    "        model = model_from_json(model_desc)\n",
    "\n",
    "    model.load_weights('C:/testAI/Crosswalk/weights.h5')\n",
    "    print('Press [q] to quit')\n",
    "\n",
    "    while(True):\n",
    "        # Capture the frames\n",
    "        grabbed, frame = video_capture.read()\n",
    "        width = video_capture.get(cv2.CAP_PROP_FRAME_WIDTH )\n",
    "        height = video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT )\n",
    "        frame, cropped, largestRect = findTrafficSign(grabbed, frame)\n",
    "        \n",
    "           \n",
    "        if cropped is not None:\n",
    "            ret, detectedTrafficSign, score = ReadTrafficSign(cropped, model)\n",
    "            percent = score *100\n",
    "            printTrafficSign = detectedTrafficSign + ' ' + str('%.2f' % percent) + '%'             # 2f\n",
    "            cv2.putText(frame, printTrafficSign, (10,50), cv2.FONT_HERSHEY_SIMPLEX, 2, (100, 255, 100), 2)\n",
    "                                          \n",
    "        if time.time() - TIC > 5:           \n",
    "                \n",
    "            if cropped is not None:\n",
    "                 \n",
    "                if score > 0.99:\n",
    "                    if 'walk' in detectedTrafficSign:\n",
    "                        SerialSendCommand(ser, \"w\")\n",
    "                        TIC = time.time()\n",
    "                        print(\"walk\")\n",
    "\n",
    "                    elif 'stop' in detectedTrafficSign:\n",
    "                        SerialSendCommand(ser, \"s\")\n",
    "                        TIC = time.time()\n",
    "                        print(\"stop\")\n",
    "\n",
    "                    elif 'none' in detectedTrafficSign:\n",
    "                        pass \n",
    "                                                        \n",
    "                                                 \n",
    "        \n",
    "        #Display the resulting frame\n",
    "        cv2.imshow('frame', frame)\n",
    "\n",
    "        # Handle user keyboard inputs\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            print('User quit program...')\n",
    "            break\n",
    "        \n",
    "            \n",
    "   \n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
